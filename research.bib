% Encoding: UTF-8

@Article{kaul2020inference,
  author   = {Kaul, Abhishek and Fotopoulos, Stergios B and Jandhyala, Venkata K and Safikhani, Abolfazl},
  journal  = {arXiv preprint arXiv:2007.01888},
  title    = {Inference on the change point in high dimensional time series models via plug in least square},
  year     = {2020},
  abstract = {We study a plug in least squares estimator for the change point parameter where change is in the mean of a high dimensional random vector under subgaussian or subexponential distributions. We obtain sufficient conditions under which this estimator possesses sufficient adaptivity against plug in estimates of mean parameters in order to yield an optimal rate of convergence $O_p(\xi^{-2})$ in the integer scale. This rate is preserved while allowing high dimensionality as well as a potentially diminishing jump size $\xi,$ provided $s\log (p\vee T)=o(\surd(Tl_T))$ or $s\log^{3/2}(p\vee T)=o(\surd(Tl_T))$ in the subgaussian and subexponential cases, respectively. Here $s,p,T$ and $l_T$ represent a sparsity parameter, model dimension, sampling period and the separation of the change point from its parametric boundary, respectively. Moreover, since the rate of convergence is free of $s,p$ and logarithmic terms of $T,$ it allows the existence of limiting distributions under high dimensional asymptotics. These distributions are then derived as the {\it argmax} of a two sided negative drift Brownian motion or a two sided negative drift random walk under vanishing and non-vanishing jump size regimes, respectively, thereby allowing inference on the change point parameter. Feasible algorithms for implementation of the proposed methodology are provided. Theoretical results are supported with monte-carlo simulations.},
  file     = {:/Users/abhishekkaul/Documents/Publications bib/arxiv_ejs.pdf:PDF},
  keywords = {change point, inference, high dimensions, limiting distribution.},
  url      = {media/arxiv_ejs.pdf},
}

@Article{kaul2020graphical,
  author   = {Kaul, Abhishek and Zhang, Hongjin and Tsampourakis, Konstantinos},
  journal  = {arXiv preprint arXiv:2005.09711},
  title    = {Inference on the Change Point in High Dimensional Dynamic Graphical Models},
  year     = {2020},
  abstract = {We propose a new estimator for the change point parameter in a dynamic high dimensional graphical model setting. We show that the proposed estimator retains sufficient adaptivity against plugin estimates of the edge structure of the underlying graphical models, in order to yield an $O(\psi^{-2})$ rate of convergence of the change point estimator in the integer scale. This rate is preserved while allowing high dimensionality as well as a diminishing jump size $\psi,$ provided $s\log^{3/2}(p\vee T)=o\big(\surd(Tl_T)\big).$ Here $s,p,T$ and $l_T$ represent a sparsity parameter, model dimension, sampling period and the separation of the change point from its parametric boundary, respectively. Moreover, since the rate of convergence is free of $s,p$ and logarithmic terms of $T,$ it allows the existence of a limiting distribution valid in the high dimensional setting, which is then derived. The method does not assume an underlying Gaussian distribution. Theoretical results are supported numerically with monte carlo simulations.},
  keywords = {High dimensions, dynamic graphical models, change point, inference, limiting distribution.},
  url      = {media/arxiv_graphical.pdf},
}

@Article{kaul2017structural,
  author    = {Kaul, Abhishek and Davidov, Ori and Peddada, Shyamal D},
  journal   = {Biostatistics},
  title     = {Structural zeros in high-dimensional data with applications to microbiome studies},
  year      = {2017},
  number    = {3},
  pages     = {422--433},
  volume    = {18},
  abstract  = {This paper is motivated by the recent interest in the analysis of high-dimensional microbiome data. A key feature of these data is the presence of “structural zeros” which are microbes missing from an observation vector due to an underlying biological process and not due to error in measurement. Typical notions of missingness are unable to model these structural zeros. We define a general framework which allows for structural zeros in the model and propose methods of estimating sparse high-dimensional covariance and precision matrices under this setup. We establish error bounds in the spectral and Frobenius norms for the proposed estimators and empirically verify them with a simulation study. The proposed methodology is illustrated by applying it to the global gut microbiome data of Yatsunenko and others (2012. Human gut microbiome viewed across age and geography. Nature 486, 222–227). Using our methodology we classify subjects according to the geographical location on the basis of their gut microbiome.},
  doi       = {10.1093/biostatistics/kxw053},
  file      = {:C\:/Users/akaul/Downloads/biostat.pdf:PDF},
  keywords  = {Classification, High dimension, Microbiome data, Missing data, Sparsity.},
  publisher = {Oxford University Press},
  url       = {media/biostat.pdf},
}

@Article{kaul2019efficient,
  author   = {Kaul, Abhishek and Jandhyala, Venkata K and Fotopoulos, Stergios B},
  journal  = {Journal of Machine Learning Research},
  title    = {An Efficient Two Step Algorithm for High Dimensional Change Point Regression Models Without Grid Search.},
  year     = {2019},
  number   = {111},
  pages    = {1--40},
  volume   = {20},
  abstract = {We propose a two step algorithm based on $\ell_1/\ell_0$ regularization for the detection and estimation of parameters of a high dimensional change point regression model and provide the corresponding rates of convergence for the change point as well as the regression parameter estimates. Importantly, the computational cost of our estimator is only $2\cdotp$Lasso$(n,p)$, where Lasso$(n,p)$ represents the computational burden of one Lasso optimization in a model of size $(n,p)$. In comparison, existing grid search based approaches to this problem require a computational cost of at least $n\cdot {\rm Lasso}(n,p)$ optimizations. Additionally, the proposed method is shown to be able to consistently detect the case of `no change', i.e., where no finite change point exists in the model. We work under a subgaussian random design where the underlying assumptions in our study are milder than those currently assumed in the high dimensional change point regression literature. We allow the true change point parameter $\tau_0$ to possibly move to the boundaries of its parametric space, and the jump size $\|\b_0-\g_0\|_2$ to possibly diverge as $n$ increases. We then characterize the corresponding effects on the rates of convergence of the change point and regression estimates. In particular, we show that, while an increasing jump size may have a beneficial effect on the change point estimate, however the optimal rate of regression parameter estimates are preserved only upto a certain rate of the increasing jump size. This behavior in the rate of regression parameter estimates is unique to high dimensional change point regression models only. Simulations are performed to empirically evaluate performance of the proposed estimators. The methodology is applied to community level socio-economic data of the U.S., collected from the 1990 U.S. census and other sources.},
  keywords = {Change point regression, High dimensional models, L1-L0regularization, Rate of convergence, Two phase regression},
  url      = {media/jmlr.pdf},
}

@Article{kaul2015weighted,
  author    = {Kaul, Abhishek and Koul, Hira L},
  journal   = {Journal of Multivariate Analysis},
  title     = {Weighted $\ell_1$-penalized corrected quantile regression for high dimensional measurement error models},
  year      = {2015},
  pages     = {72--91},
  volume    = {140},
  abstract  = {Standard formulations of prediction problems in high dimension regression models assume the availability of fully observed covariates and sub-Gaussian and homogeneous model errors. This makes these methods inapplicable to measurement errors models where covariates are unobservable and observations are possibly non sub-Gaussian and heterogeneous. We propose a weighted penalized corrected quantile estimator for regression parameters in linear regression models with additive measurement errors, where unobservable covariate is nonrandom. The proposed estimators forgo the need for the above mentioned model assumptions. We study these estimators in a high dimensional sparse setup where the dimensionality can grow exponentially with the sample size. We provide bounds for the statistical error associated with the estimation, that hold with asymptotic probability 1, thereby providing the $\ell_1$-consistency of the proposed estimator. We also establish the model selection consistency in terms of the correctly estimated zero components of the parameter vector. A simulation study that investigates the finite sample accuracy of the proposed estimator is also included in the paper.},
  doi       = {http://dx.doi.org/10.1016/j.jmva.2015.04.009},
  file      = {:C\:/Users/akaul/Downloads/1-s2.0-S0047259X1500113X-main.pdf:PDF},
  keywords  = {ℓ1-consistency, Model selection consistency},
  publisher = {Elsevier},
  url       = {media/jmva.pdf},
}

@Article{belloni2017pivotal,
  author   = {Belloni, Alexandre and Kaul, Abhishek and Rosenbaum, Mathieu},
  journal  = {arXiv preprint arXiv:1708.08353},
  title    = {Pivotal estimation via self-normalization for high-dimensional linear models with error in variables},
  year     = {2017},
  abstract = {We propose a new estimator for the high-dimensional linear regression model with observation error in the design where the number of coefficients is potentially larger than the sample size. The main novelty of our procedure is that the choice of penalty parameters is pivotal. The estimator is based on applying a self-normalization to the constraints that characterize the estimator. Importantly, we show how to cast the computation of the estimator as the solution of a convex program with second order cone constraints. This allows the use of algorithms with theoretical guarantees and reliable implementation. Under sparsity assumptions, we derive $\ell_q$-rates of convergence and show that consistency can be achieved even if the number of regressors exceeds the sample size. We further provide a simple to implement rule to threshold the estimator that yields a provably sparse estimator with similar $\ell_2$ and $\ell_1$-rates of convergence. The thresholds are data-driven and component dependents. Finally, we also study the rates of convergence of estimators that refit the data based on a selected support with possible model selection mistakes. In addition to our finite sample theoretical results that allow for non-i.i.d. data, we also present simulations to compare the performance of the proposed estimators.},
  keywords = {pivotal estimation, self-normalized sums, error in measurement, highdimensional models},
  url      = {media/pivotal.pdf},
}

@Article{belloni2017confidence,
  author   = {Belloni, Alexandre and Chernozhukov, Victor and Kaul, Abhishek},
  journal  = {arXiv preprint arXiv:1703.00469},
  title    = {Confidence bands for coefficients in high dimensional linear models with error-in-variables},
  year     = {2017},
  abstract = {We study high-dimensional linear models with error-in-variables. Such models are motivated by various applications in econometrics, finance and genetics. These models are challenging because of the need to account for measurement errors to avoid non-vanishing biases in addition to handle the high dimensionality of the parameters. A recent growing literature has proposed various estimators that achieve good rates of convergence. Our main contribution complements this literature with the construction of simultaneous confidence regions for the parameters of interest in such high-dimensional linear models with error-in-variables.
These confidence regions are based on the construction of moment conditions that have an additional orthogonal property with respect to nuisance parameters. We provide a construction that requires us to estimate an additional high-dimensional linear model with error-in-variables for each component of interest. We use a multiplier bootstrap to compute critical values for simultaneous confidence intervals for a subset S of the components. We show its validity despite of possible model selection mistakes, and allowing for the cardinality of S to be larger than the sample size.
We apply and discuss the implications of our results to two examples and conduct Monte Carlo simulations to illustrate the performance of the proposed procedure.},
  keywords = {honest confidence regions, error-in-variables, high dimensionalmodels},
  url      = {media/confidencebands.pdf},
}

@Article{kaul2017analysis,
  author    = {Kaul, Abhishek and Mandal, Siddhartha and Davidov, Ori and Peddada, Shyamal D},
  journal   = {Frontiers in microbiology},
  title     = {Analysis of microbiome data in the presence of excess zeros},
  year      = {2017},
  pages     = {2114},
  volume    = {8},
  abstract  = {Motivation: An important feature of microbiome count data is the presence of a large number of zeros. A common strategy to handle these excess zeros is to add a small number called pseudo-count (e.g., 1). Other strategies include using various probability models to model the excess zero counts. Although adding a pseudo-count is simple and widely used, as demonstrated in this paper, it is not ideal. On the other hand, methods that model excess zeros using a probability model often make an implicit assumption that all zeros can be explained by a common probability models. As described in this article, this is not always recommended as there are potentially three types/sources of zeros in a microbiome data. The purpose of this paper is to develop a simple methodology to identify and accomodate three different types of zeros and to test hypotheses regarding the relative abundance of taxa in two or more experimental groups. Another major contribution of this paper is to perform constrained (directional or ordered) inference when there are more than two ordered experimental groups (e.g., subjects ordered by diet or age groups or environmental exposure groups). As far as we know this is the first paper that addresses such problems in the analysis of microbiome data.

Results: Using extensive simulation studies, we demonstrate that the proposed methodology not only controls the false discovery rate at a desired level of significance while competing well in terms of power with DESeq2, a popular procedure derived from RNASeq literature. As expected, the method using pseudo-counts tends to be very conservative and the classical t-test that ignores the underlying simplex structure in the data has an inflated FDR.},
  doi       = {10.3389/fmicb.2017.02114},
  keywords  = {Microbiome data, Aitchisons log-ratio, bootstrap, covariates, cross-sectional data, false discovery rate (FDR)},
  publisher = {Frontiers},
  url       = {media/zeros.pdf},
}

@Article{lane2019household,
  author    = {Lane, Avery A and others},
  journal   = {American journal of physical anthropology},
  title     = {Household composition and the infant fecal microbiome: The INSPIRE study},
  year      = {2019},
  number    = {3},
  pages     = {526--539},
  volume    = {169},
  abstract  = {Objectives
Establishment and development of the infant gastrointestinal microbiome (GIM) varies cross‐culturally and is thought to be influenced by factors such as gestational age, birth mode, diet, and antibiotic exposure. However, there is little data as to how the composition of infants' households may play a role, particularly from a cross‐cultural perspective. Here, we examined relationships between infant fecal microbiome (IFM) diversity/composition and infants' household size, number of siblings, and number of other household members.

Materials and methods
We analyzed 377 fecal samples from healthy, breastfeeding infants across 11 sites in eight different countries (Ethiopia, The Gambia, Ghana, Kenya, Peru, Spain, Sweden, and the United States). Fecal microbial community structure was determined by amplifying, sequencing, and classifying (to the genus level) the V1–V3 region of the bacterial 16S rRNA gene. Surveys administered to infants' mothers identified household members and composition.

Results
Our results indicated that household composition (represented by the number of cohabitating siblings and other household members) did not have a measurable impact on the bacterial diversity, evenness, or richness of the IFM. However, we observed that variation in household composition categories did correspond to differential relative abundances of specific taxa, namely: Lactobacillus, Clostridium, Enterobacter, and Klebsiella.

Discussion
This study, to our knowledge, is the largest cross‐cultural study to date examining the association between household composition and the IFM. Our results indicate that the social environment of infants (represented here by the proxy of household composition) may influence the bacterial composition of the infant GIM, although the mechanism is unknown. A higher number and diversity of cohabitants and potential caregivers may facilitate social transmission of beneficial bacteria to the infant gastrointestinal tract, by way of shared environment or through direct physical and social contact between the maternal–infant dyad and other household members. These findings contribute to the discussion concerning ways by which infants are influenced by their social environments and add further dimensionality to the ongoing exploration of social transmission of gut microbiota and the “old friends” hypothesis.},
  doi       = {10.1002/ajpa.23843},
  publisher = {Wiley Online Library},
  url       = {media/avery_microbiome.pdf},
}

@Article{macneill2020multiple,
  author    = {MacNeill, IB and Jandhyala, VK and Kaul, A and Fotopoulos, SB},
  journal   = {Environmetrics},
  title     = {Multiple change-point models for time series},
  year      = {2020},
  number    = {1},
  pages     = {e2593},
  volume    = {31},
  abstract  = {The “Bayes‐type” method of deriving change‐point test statistics was introduced by Chernoff and Zacks (1964). Other authors subsequently adapted this approach and derived Bayes‐type statistics for at most one change (AMOC), and for multiple change points, under a variety of model formulations. Asymptotic distribution theory has always been limited to the AMOC statistics because of the perceived complexity of multiple change‐point statistics. In this article, it is shown that the Bayes‐type statistic derived to test for multiple change points is directly proportional to the AMOC statistic. This result immediately provides distributional results for Bayes‐type multiple change‐point statistics. In addition, it fundamentally alters the current understanding of the AMOC statistic. It follows from this result that the Bayes‐type statistic derived under AMOC conditions in fact tests for at least one change (ALOC), even though the statistic is derived under AMOC formulation. Under asymptotic consideration, the result also extends to the case of model errors following a stationary process. As an example, the classical Nile River data are revisited and analyzed for the presence of multiple change points.},
  doi       = {10.1002/env.2593},
  keywords  = {at least one change, at most one change, Bayes-type tests, multiple change-points, Nile River series},
  publisher = {Wiley Online Library},
  url       = {media/environmetrics.pdf},
}

@Article{kaul2014lasso,
  author    = {Kaul, Abhishek},
  journal   = {Journal of Statistical Planning and Inference},
  title     = {Lasso with long memory regression errors},
  year      = {2014},
  pages     = {11--26},
  volume    = {153},
  abstract  = {Lasso is a computationally efficient approach to model selection and estimation, and its properties are well studied when the regression errors are independent and identically distributed. We study the case, where the regression errors form a long memory moving average process. We establish a finite sample oracle inequality for the Lasso solution. We then show the asymptotic sign consistency in this setup. These results are established in the high dimensional setup (p> n) where p can be increasing exponentially with n. Finally, we show the consistency, n 1/2− d-consistency of Lasso, along with the oracle property of adaptive Lasso, in the case where p is fixed. Here d is the memory parameter of the stationary error sequence. The performance of Lasso is also analysed in the present setup with a simulation study.},
  doi       = {10.1016/j.jspi.2014.05.003},
  keywords  = {Sparsity, Long memory dependence, Sign consistency, Asymptotic normality},
  publisher = {Elsevier},
  url       = {media/jspi.pdf},
}

@PhdThesis{kaulthesis,
  author    = {Kaul, Abhishek},
  title     = {High dimensional linear regression models under long memory dependence and measurement error},
  year      = {2015},
  abstract  = {This dissertation consists of three chapters. The first chapter introduces the models under consideration and motivates problems of interest. A brief literature review is also provided in this chapter.

The second chapter investigates the properties of Lasso under long range dependent model errors. Lasso is a computationally efficient approach to model selection and estimation, and its properties are well studied when the regression errors are independent and identically distributed. We study the case, where the regression errors form a long memory moving average process. We establish a finite sample oracle inequality for the Lasso solution. We then show the asymptotic sign consistency in this setup. These results are established in the high dimensional setup (p > n) where p can be increasing exponentially with n. Finally, we show the consistency, n^{1/2-d}-consistency of Lasso, along with the oracle property of adaptive asso, in the case where p is fixed. Here d is the memory parameter of the stationary error sequence. The performance of Lasso is also analysed in the present setup with a simulation study. 

The third chapter proposes and investigates the properties of a penalized quantile based estimator for measurement error models. Standard formulations of prediction problems in high dimension regression models assume the availability of fully observed covariates and sub-Gaussian and homogenous model errors. This makes these methods inapplicable to measurement errors models where covariates are unobservable and observations are possibly non sub-Gaussian and heterogeneous. We propose weighted penalized corrected quantile estimators for the regression parameter vector in linear regression models with additive measurement errors, where unobservable covariates are nonrandom. The proposed estimators forgo the need for the above mentioned model assumptions. We study these estimators in both the fixed dimension and high dimensional sparse setups, in the latter setup, the dimensionality can grow exponentially with the sample size. In the fixed dimensional setting we provide the oracle properties associated with the proposed estimators. In the high dimensional setting, we provide bounds for the statistical error associated with the estimation, that hold with asymptotic probability 1, thereby providing the  L1-consistency of the proposed estimator. We also establish the model selection consistency in terms of the correctly estimated zero components of the parameter vector. A simulation study that investigates the finite sample accuracy of the proposed estimator is also included in this chapter.},
  publisher = {Michigan State University},
  url       = {media/thesis.pdf},
}

@Comment{jabref-meta: databaseType:bibtex;}
