---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Lasso with long memory regression errors
subtitle: ''
summary: ''
authors:
- Abhishek Kaul
tags:
- '"Sparsity"'
- '"Long memory dependence"'
- '"Sign consistency"'
- '"Asymptotic normality"'
categories: []
date: '2014-01-01'
lastmod: 2020-11-12T20:43:24-08:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2020-11-13T04:43:23.670961Z'
publication_types:
- '2'
abstract: Lasso is a computationally efficient approach to model selection and estimation,
  and its properties are well studied when the regression errors are independent and
  identically distributed. We study the case, where the regression errors form a long
  memory moving average process. We establish a finite sample oracle inequality for
  the Lasso solution. We then show the asymptotic sign consistency in this setup.
  These results are established in the high dimensional setup (p> n) where p can be
  increasing exponentially with n. Finally, we show the consistency, n 1/2âˆ’ d-consistency
  of Lasso, along with the oracle property of adaptive Lasso, in the case where p
  is fixed. Here d is the memory parameter of the stationary error sequence. The performance
  of Lasso is also analysed in the present setup with a simulation study.
publication: '*Journal of Statistical Planning and Inference*'
url_pdf: media/jspi.pdf
doi: 10.1016/j.jspi.2014.05.003
---
