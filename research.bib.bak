% Encoding: UTF-8

@Article{kaul2020inference,
  author   = {Kaul, Abhishek and Fotopoulos, Stergios B and Jandhyala, Venkata K and Safikhani, Abolfazl},
  title    = {Inference on the change point in high dimensional time series models via plug in least square},
  journal  = {arXiv preprint arXiv:2007.01888},
  year     = {2020},
  abstract = {We study a plug in least squares estimator for the change point parameter where change is in the mean of a high dimensional random vector under subgaussian or subexponential distributions. We obtain sufficient conditions under which this estimator possesses sufficient adaptivity against plug in estimates of mean parameters in order to yield an optimal rate of convergence $O_p(\xi^{-2})$ in the integer scale. This rate is preserved while allowing high dimensionality as well as a potentially diminishing jump size $\xi,$ provided $s\log (p\vee T)=o(\surd(Tl_T))$ or $s\log^{3/2}(p\vee T)=o(\surd(Tl_T))$ in the subgaussian and subexponential cases, respectively. Here $s,p,T$ and $l_T$ represent a sparsity parameter, model dimension, sampling period and the separation of the change point from its parametric boundary, respectively. Moreover, since the rate of convergence is free of $s,p$ and logarithmic terms of $T,$ it allows the existence of limiting distributions under high dimensional asymptotics. These distributions are then derived as the {\it argmax} of a two sided negative drift Brownian motion or a two sided negative drift random walk under vanishing and non-vanishing jump size regimes, respectively, thereby allowing inference on the change point parameter. Feasible algorithms for implementation of the proposed methodology are provided. Theoretical results are supported with monte-carlo simulations.},
}

@Article{kaul2020graphical,
  author   = {Kaul, Abhishek and Zhang, Hongjin and Tsampourakis, Konstantinos},
  title    = {Inference on the Change Point in High Dimensional Dynamic Graphical Models},
  journal  = {arXiv preprint arXiv:2005.09711},
  year     = {2020},
  abstract = {We propose a new estimator for the change point parameter in a dynamic high dimensional graphical model setting. We show that the proposed estimator retains sufficient adaptivity against plugin estimates of the edge structure of the underlying graphical models, in order to yield an $O(\psi^{-2})$ rate of convergence of the change point estimator in the integer scale. This rate is preserved while allowing high dimensionality as well as a diminishing jump size $\psi,$ provided $s\log^{3/2}(p\vee T)=o\big(\surd(Tl_T)\big).$ Here $s,p,T$ and $l_T$ represent a sparsity parameter, model dimension, sampling period and the separation of the change point from its parametric boundary, respectively. Moreover, since the rate of convergence is free of $s,p$ and logarithmic terms of $T,$ it allows the existence of a limiting distribution valid in the high dimensional setting, which is then derived. The method does not assume an underlying Gaussian distribution. Theoretical results are supported numerically with monte carlo simulations.},
}

@Article{kaul2017structural,
  author    = {Kaul, Abhishek and Davidov, Ori and Peddada, Shyamal D},
  title     = {Structural zeros in high-dimensional data with applications to microbiome studies},
  journal   = {Biostatistics},
  year      = {2017},
  volume    = {18},
  number    = {3},
  pages     = {422--433},
  abstract  = {This paper is motivated by the recent interest in the analysis of high-dimensional microbiome data. A key feature of these data is the presence of “structural zeros” which are microbes missing from an observation vector due to an underlying biological process and not due to error in measurement. Typical notions of missingness are unable to model these structural zeros. We define a general framework which allows for structural zeros in the model and propose methods of estimating sparse high-dimensional covariance and precision matrices under this setup. We establish error bounds in the spectral and Frobenius norms for the proposed estimators and empirically verify them with a simulation study. The proposed methodology is illustrated by applying it to the global gut microbiome data of Yatsunenko and others (2012. Human gut microbiome viewed across age and geography. Nature 486, 222–227). Using our methodology we classify subjects according to the geographical location on the basis of their gut microbiome.},
  doi       = {10.1093/biostatistics/kxw053},
  file      = {:C\:/Users/akaul/Downloads/biostat.pdf:PDF},
  keywords  = {Classification, High dimension, Microbiome data, Missing data, Sparsity.},
  publisher = {Oxford University Press},
  url       = {https://academic.oup.com/biostatistics/article/18/3/422/2870656},
}

@Article{kaul2019efficient,
  author   = {Kaul, Abhishek and Jandhyala, Venkata K and Fotopoulos, Stergios B},
  title    = {An Efficient Two Step Algorithm for High Dimensional Change Point Regression Models Without Grid Search.},
  journal  = {Journal of Machine Learning Research},
  year     = {2019},
  volume   = {20},
  number   = {111},
  pages    = {1--40},
  abstract = {	We propose a two step algorithm based on $\ell_1/\ell_0$ regularization for the detection and estimation of parameters of a high dimensional change point regression model and provide the corresponding rates of convergence for the change point as well as the regression parameter estimates. Importantly, the computational cost of our estimator is only $2\cdotp$Lasso$(n,p)$, where Lasso$(n,p)$ represents the computational burden of one Lasso optimization in a model of size $(n,p)$. In comparison, existing grid search based approaches to this problem require a computational cost of at least $n\cdot {\rm Lasso}(n,p)$ optimizations. Additionally, the proposed method is shown to be able to consistently detect the case of `no change', i.e., where no finite change point exists in the model. We work under a subgaussian random design where the underlying assumptions in our study are milder than those currently assumed in the high dimensional change point regression literature. We allow the true change point parameter $\tau_0$ to possibly move to the boundaries of its parametric space, and the jump size $\|\b_0-\g_0\|_2$ to possibly diverge as $n$ increases. We then characterize the corresponding effects on the rates of convergence of the change point and regression estimates. In particular, we show that, while an increasing jump size may have a beneficial effect on the change point estimate, however the optimal rate of regression parameter estimates are preserved only upto a certain rate of the increasing jump size. This behavior in the rate of regression parameter estimates is unique to high dimensional change point regression models only. Simulations are performed to empirically evaluate performance of the proposed estimators. The methodology is applied to community level socio-economic data of the U.S., collected from the 1990 U.S. census and other sources.},
}

@Article{kaul2015weighted,
  author    = {Kaul, Abhishek and Koul, Hira L},
  title     = {Weighted $\ell_1$-penalized corrected quantile regression for high dimensional measurement error models},
  journal   = {Journal of Multivariate Analysis},
  year      = {2015},
  volume    = {140},
  pages     = {72--91},
  abstract  = {Standard formulations of prediction problems in high dimension regression models assume the availability of fully observed covariates and sub-Gaussian and homogeneous model errors. This makes these methods inapplicable to measurement errors models where covariates are unobservable and observations are possibly non sub-Gaussian and heterogeneous. We propose a weighted penalized corrected quantile estimator for regression parameters in linear regression models with additive measurement errors, where unobservable covariate is nonrandom. The proposed estimators forgo the need for the above mentioned model assumptions. We study these estimators in a high dimensional sparse setup where the dimensionality can grow exponentially with the sample size. We provide bounds for the statistical error associated with the estimation, that hold with asymptotic probability 1, thereby providing the $\ell_1$-consistency of the proposed estimator. We also establish the model selection consistency in terms of the correctly estimated zero components of the parameter vector. A simulation study that investigates the finite sample accuracy of the proposed estimator is also included in the paper.},
  doi       = {http://dx.doi.org/10.1016/j.jmva.2015.04.009},
  file      = {:C\:/Users/akaul/Downloads/1-s2.0-S0047259X1500113X-main.pdf:PDF},
  keywords  = {ℓ1-consistency, Model selection consistency},
  publisher = {Elsevier},
  url       = {https://www.sciencedirect.com/science/article/pii/S0047259X1500113X?via%3Dihub},
}

@Article{belloni2017pivotal,
  author   = {Belloni, Alexandre and Kaul, Abhishek and Rosenbaum, Mathieu},
  title    = {Pivotal estimation via self-normalization for high-dimensional linear models with error in variables},
  journal  = {arXiv preprint arXiv:1708.08353},
  year     = {2017},
  abstract = {We propose a new estimator for the high-dimensional linear regression model with observation error in the design where the number of coefficients is potentially larger than the sample size. The main novelty of our procedure is that the choice of penalty parameters is pivotal. The estimator is based on applying a self-normalization to the constraints that characterize the estimator. Importantly, we show how to cast the computation of the estimator as the solution of a convex program with second order cone constraints. This allows the use of algorithms with theoretical guarantees and reliable implementation. Under sparsity assumptions, we derive $\ell_q$-rates of convergence and show that consistency can be achieved even if the number of regressors exceeds the sample size. We further provide a simple to implement rule to threshold the estimator that yields a provably sparse estimator with similar $\ell_2$ and $\ell_1$-rates of convergence. The thresholds are data-driven and component dependents. Finally, we also study the rates of convergence of estimators that refit the data based on a selected support with possible model selection mistakes. In addition to our finite sample theoretical results that allow for non-i.i.d. data, we also present simulations to compare the performance of the proposed estimators.},
}

@Article{belloni2017confidence,
  author   = {Belloni, Alexandre and Chernozhukov, Victor and Kaul, Abhishek},
  title    = {Confidence bands for coefficients in high dimensional linear models with error-in-variables},
  journal  = {arXiv preprint arXiv:1703.00469},
  year     = {2017},
  abstract = {We study high-dimensional linear models with error-in-variables. Such models are motivated by various applications in econometrics, finance and genetics. These models are challenging because of the need to account for measurement errors to avoid non-vanishing biases in addition to handle the high dimensionality of the parameters. A recent growing literature has proposed various estimators that achieve good rates of convergence. Our main contribution complements this literature with the construction of simultaneous confidence regions for the parameters of interest in such high-dimensional linear models with error-in-variables.
These confidence regions are based on the construction of moment conditions that have an additional orthogonal property with respect to nuisance parameters. We provide a construction that requires us to estimate an additional high-dimensional linear model with error-in-variables for each component of interest. We use a multiplier bootstrap to compute critical values for simultaneous confidence intervals for a subset $S$ of the components. We show its validity despite of possible model selection mistakes, and allowing for the cardinality of $S$ to be larger than the sample size.
We apply and discuss the implications of our results to two examples and conduct Monte Carlo simulations to illustrate the performance of the proposed procedure.},
}

@Article{kaul2017analysis,
  author    = {Kaul, Abhishek and Mandal, Siddhartha and Davidov, Ori and Peddada, Shyamal D},
  title     = {Analysis of microbiome data in the presence of excess zeros},
  journal   = {Frontiers in microbiology},
  year      = {2017},
  volume    = {8},
  pages     = {2114},
  abstract  = {An important feature of microbiome count data is the presence of a large number of zeros. A common strategy to handle these excess zeros is to add a small number called pseudo-count (e.g., 1). Other strategies include using various probability models to model the excess zero counts. Although adding a pseudo-count is simple and widely used, as demonstrated in this paper, it is not ideal. On the other hand, methods that model excess zeros using a probability model often make an implicit assumption that all zeros can be explained by a common probability models. As described in this article, this is not always recommended as there are potentially three types/sources of zeros in a microbiome data. The purpose of this paper is to develop a simple methodology to identify and accomodate three different types of zeros and to test hypotheses regarding the relative abundance of taxa in two or more experimental groups. Another major contribution of this paper is to perform constrained (directional or ordered) inference when there are more than two ordered experimental groups (e.g., subjects ordered by diet or age groups or environmental exposure groups). As far as we know this is the first paper that addresses such problems in the analysis of microbiome data.},
  publisher = {Frontiers},
}

@Article{lane2019household,
  author    = {Lane, Avery A and others},
  title     = {Household composition and the infant fecal microbiome: The INSPIRE study},
  journal   = {American journal of physical anthropology},
  year      = {2019},
  volume    = {169},
  number    = {3},
  pages     = {526--539},
  abstract  = {Establishment and development of the infant gastrointestinal microbiome (GIM) varies cross‐culturally and is thought to be influenced by factors such as gestational age, birth mode, diet, and antibiotic exposure. However, there is little data as to how the composition of infants' households may play a role, particularly from a cross‐cultural perspective. Here, we examined relationships between infant fecal microbiome (IFM) diversity/composition and infants' household size, number of siblings, and number of other household members.},
  publisher = {Wiley Online Library},
}

@Article{macneill2020multiple,
  author    = {MacNeill, IB and Jandhyala, VK and Kaul, A and Fotopoulos, SB},
  title     = {Multiple change-point models for time series},
  journal   = {Environmetrics},
  year      = {2020},
  volume    = {31},
  number    = {1},
  pages     = {e2593},
  abstract  = {The “Bayes‐type” method of deriving change‐point test statistics was introduced by Chernoff and Zacks (1964). Other authors subsequently adapted this approach and derived Bayes‐type statistics for at most one change (AMOC), and for multiple change points, under a variety of model formulations. Asymptotic distribution theory has always been limited to the AMOC statistics because of the perceived complexity of multiple change‐point statistics. In this article, it is shown that the Bayes‐type statistic derived to test for multiple change points is directly proportional to the AMOC statistic. This result immediately provides distributional results for Bayes‐type multiple change‐point statistics. In addition, it fundamentally alters the current understanding of the AMOC statistic. It follows from this result that the Bayes‐type statistic derived under AMOC conditions in fact tests for at least one change (ALOC), even though the statistic is derived under AMOC formulation. Under asymptotic consideration, the result also extends to the case of model errors following a stationary process. As an example, the classical Nile River data are revisited and analyzed for the presence of multiple change points.},
  publisher = {Wiley Online Library},
}

@Article{kaul2014lasso,
  author    = {Kaul, Abhishek},
  title     = {Lasso with long memory regression errors},
  journal   = {Journal of Statistical Planning and Inference},
  year      = {2014},
  volume    = {153},
  pages     = {11--26},
  abstract  = {Lasso is a computationally efficient approach to model selection and estimation, and its properties are well studied when the regression errors are independent and identically distributed. We study the case, where the regression errors form a long memory moving average process. We establish a finite sample oracle inequality for the Lasso solution. We then show the asymptotic sign consistency in this setup. These results are established in the high dimensional setup $(p> n)$ where $p$ can be increasing exponentially with $n.$ Finally, we show the consistency, $n^(1/2− d)$-consistency of Lasso, along with the oracle property of adaptive Lasso, in the case where $p$ is fixed. Here $d$ is the memory parameter of the stationary error sequence. The performance of Lasso is also analysed in the present setup with a simulation study.},
  publisher = {Elsevier},
}

@Comment{jabref-meta: databaseType:bibtex;}
